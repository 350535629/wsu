---
title: "Cpts575 Hw5"
author: "Mengxiao Zhang"
output: pdf_document
---
# Part 1
##  a. Estimate the probability.
The predict function is :
$$\hat{p}=\frac{e^{-7+0.1X_1+X_2-0.04X_3}}{1+e^{-7+0.1X_1+X_2-0.04X_3}}$$
$$\hat{p}(X_1=32,X_2=3.0,X_3=12)=0.2175502$$
So, the probability of gets an A in the class is 21.75502%

##  b. How many hours would the student need to study.
According to the question a, we have the predict function and we need to 
let the predict equal to at least 0.5.
$$\hat{p}(X_1=a, x_2=3.0, x_3=12)=0.5$$
$$\Rightarrow\frac{e^{-7+0.1a+3.0-0.04*12}}{1+e^{-7+0.1a_1+3.0-0.04*12}} = 0.5$$
$$\Rightarrow e^{-7+0.1a+3.0-0.04*12} = 1$$
$$\Rightarrow -7+0.1a+3.0-0.04*12 = 0$$
$$\Rightarrow a = 44.8$$
So, the student is part(a) needs to study 44.8 hours to have 50% chance of getting an A.

##  c. How many hours would a student with 3.0 GPA and a PSQI score of 3 need to have a 50% chance of getting an A.
$$\hat{p}(X_1=a, x_2=3.0, x_3=3)=0.5$$

$$\Rightarrow\frac{e^{-7+0.1a+3.0-0.04*3}}{1+e^{-7+0.1a_1+3.0-0.04*3}} = 0.5$$
$$\Rightarrow -7+0.1a+3.0-0.04*3 = 0$$
$$\Rightarrow a = 41.2$$
So, the students with 3.0 GPA and PSQI score of 3 need to study 41.2 hours to have 50% chance of getting an A.

# Part 2
##  a. Data Collection.
```{r}
library(RJSONIO)
sections = c("world", "science", "business", "technology", "sport", "environment")
mydata = setNames(data.frame(matrix(ncol = 4, nrow = 0)),
                  c("id", "title", "body", "section"))
n = 0
dataset = list()
for (page in 1:10){
    for (s in 1:length(sections)){
        url = paste("http://content.guardianapis.com/search?section=", sections[s], "&lang=en&show-fields=body&page=", page, "&page-size=120&api-key=14453543-e613-4acb-a757-e79000c47ca9", sep="")
        data = fromJSON(url)$response$results
        for (i in 1:120){
            if (data[[i]]$type == "article"){
                mydata[nrow(mydata)+1,] = c(data[[i]]$id, data[[i]]$webTitle, data[[i]]$fields, data[[i]]$sectionId)
            }
        }
    }
}
mydata[1,]$title
```
 
## b. Data cleaning.
```{r}
for (i in 1:nrow(mydata)){
    Encoding(mydata[i,][["body"]]) <- "UTF-8"
    Encoding(mydata[i,][["title"]]) <- "UTF-8?gsub"
    mydata[i,]$body <- gsub("<.*?>", "", mydata[i,][["body"]])
    mydata[i,]$body <- gsub("[^[:alnum:][:space:]]",
"", mydata[i,][["body"]])
    mydata[i,]$body <- tolower(mydata[i,][["body"]])
}
strwrap(mydata[432,]$body, width=80)
```

## c. Tokenization.
```{r}
library(tm)
TermMatrix = DocumentTermMatrix(Corpus(VectorSource(mydata$body)),
                                control=list(removeNumbers=TRUE,
                                                stopwords=TRUE,
                                                stemming=TRUE))
as.matrix(TermMatrix[32, which(as.matrix(TermMatrix[32, ]) != 0)])
```

## d. Classification.
```{r}
library(e1071)
library(caret)
TermMatrix = removeSparseTerms(TermMatrix, 0.99)

cor_matrix = cor(as.matrix(TermMatrix))
cor_terms = sort(findCorrelation(cor_matrix, cutoff = 0.9))
TermMatrix = TermMatrix[, -c(cor_terms)]

TM_train = TermMatrix[1:floor(nrow(TermMatrix)*0.8),]
TM_test = TermMatrix[(floor(nrow(TermMatrix)*0.8)+1):nrow(TermMatrix),]
data_train = mydata[1:floor(nrow(mydata)*0.8), ]
data_test = mydata[(floor(nrow(mydata)*0.8)+1):nrow(mydata), ]
data_train$section = as.factor(data_train$section)
data_test$section = as.factor(data_test$section)

m = naiveBayes(as.matrix(TM_train), data_train$section)
p = predict(m, as.matrix(TM_test))
confusionMatrix(p, data_test$section)
```
